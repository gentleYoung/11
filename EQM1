5.8 EGRESS QUEUE MANAGER(EQM )
Egress Queue Management（EQM）模块接收PE送过来的包信息，控制报文数据的存储，基于输出队列进行缓存管理和调度，完成PON口的入队DBA信息发送给EPS_PON。
5.8.1 Main Features
 支持管理报文数据的存储：
o 根据端口组内置缓存使用情况和相应的水线配置进行自适应内外置缓存选择；
o 支持基于出端口组固定选择内外置缓存，组播报文单独配置；
o 支持将目的缓存是EFB的报文写入外部缓存；
 基于出口进行队列管理，队列规格：256队列；
 支持片外和片内缓存：
o 最大支持32K个片外Cell；
o 最大支持6K个片内Cell；
o 每个EFB Cell 大小512BYTE/1024BYTE 可配置；
o 每个IFB Cell大小 固定160BYTE；
 每个队列可配选择队列属性模板：
o 支持32个队列属性模板；
o 支持队列独享资源数目配置；
o 支持队列最大资源数目配置；
o 支持队列组属性配置。
 支持32个队列组：每个队列可配置同属于两个队列组；
 支持三级缓存管理：
o 支持队列缓存管理；
o 支持队列组缓存管理；
o 支持整芯片缓存统计管理。
 支持48个shaper模板；
 二次入队：
o 支持专用的二次入队调度端口，调度出队的报文从EPS环回到EQM入口再次入队；
 支持通用调度器：
o 48个
o 每个通用调度器支持16个输入端；
o 每个通用调度器输入端支持上一级调度器输出或者队列输出；
o 每个通用调度器输入端调度组属性和调度权重可配置；
o 调度算法支持WRR、DRR、SP、SP+WRR/DRR，权重精度为1/255；
o 0~7号调度器支持共享带宽调度，剩余带宽支持按优先级分配；
o SP调度最大支持8个优先级；
o 输出有一个shaper进行限速；
o 使用共享带宽调度的调度器只能使用0~7输入端。
. 支持端口调度器：
o 支持4个端口调度器；
o 每个端口调度器输入端支持上一级通用调度器输出；
o 端口调度器0输入端为2个，用于二次入队，分别上、下行内部环回虚拟端口；
o 端口调度器1输入端为6个，用于4个PIE虚拟端口、1个PRBS端口和1个WOE端口；
o 端口调度器2输入端为6个，用于6个ETH端口；
o 端口调度器3输入端为16个，用于PON端口；
o 调度算法仅支持RR调度；
o 每个端口调度器上可配置shaper进行限速。
. 流量整形（shaper）：
o 支持48个shaper模板；
o 支持单漏桶；
o 支持预借令牌，最多支持一个报文预借；
o shaper速率支持0-10Gbps；
o 通用调度器shaper精度为16 Kbps；端口调度器shaper步长1Mbps；
o 通用调度器突发尺寸0-16M Bytes，步长1Bytes；
o 通用调度器令牌更新周期默认为125us，全局可配；端口调度器令牌更新周期为8us。
. PIE逻辑通道映射：
o EQM配合EPS完成PIE逻辑通道的映射。
PON DBA上报：
o 成功入队报文均上报DBA；
o 支持 报文长度（包括4Byte FCS）上报；
o 支持队列优先级上报；
 支持组播报文通过MCID管理转发；
 支持Quark 512个业务流索引统计，索引0/1统计到前256项，索引2/3统计到后256项；
 支持队列拥塞退出水线可配置；
 报文丢弃：
o 基于16BYTE CELL字节水线配置报文丢弃；
o 基于缓存CELL数目水线配置报文丢弃；
o 基于16Byte Cell水线配置报文丢弃功能支持使能配置，默认使能；
o 基于MCID总占用百分比与队列配置水线对比的报文丢弃；
o 基于MCID总占用百分比与出端口配置水线对比的报文丢弃。
 支持队列统计功能：
o 支持基于队列报文个数和字节统计，包括出队，尾丢弃和拥塞丢弃统计；报文的入队统计为出队统计加上攒在队列的报文统计的和，由软件读出进行计算。
 只支持静态修改队列属性和调度器拓扑结构；
 支持低功耗功能。
 支持报文的丢弃动作。
o PE送过来的报文已经打上drop标记，表示前级模块处理已经将该报文丢弃了，则直接丢弃。
o 组播叶子的份数超过配置的组播叶子份数限制。（PQM透传指示丢弃）
o 无可用的缓存地址，报文丢弃（仅强制模式）。
5.8.2 Terminology
The following terms have specific meanings in this section of the document.
Term Description
SP Strict Priority
RR Round Robin.
WRR Weight Round Robin
MEMP Memory Pool，IFB
QE Queue Entity
PBM Packet Buffer Manager
FAM Free Address Manager
Table 5.8-1 EQM Terminology
5.8.3 Performance
缓存管理性能为8 拍同时完成一个报文的入队及一个报文的出队。调度性能为单端口（目
的端口，对应最后一级调度器输出）16 拍，双端口及多端口（目的端口，对应最后一级
调度器输出）8 拍调度一个报文。
PBM 模块入队的最大性能为8cycle/pkt , 受PDM 写DDR 的反压，反压会逐级传递到
PE。
QE 模块入队的性能为8cycle/pkt。
QS 端口交织调度性能31.25Mpps，单端口性能调度为15.625Mpps
因此EQM的性能瓶颈在PDM子模块的DDR 写操作。若PE 入口流量大于DDR 写带宽
时，EQM会反压到PE。
5.8.4 EQM Overview
EQM的基本框图：
Figure 5.8-1 EQM Block Diagram
 Block Introduction
PBM：Packet Buffer Manager,PBM子模块从PE或EPS模块接收报文信息，用于缓存管理。缓存管理包括：队列资源管理和WRED，播叶子丢弃，冗余权重释放，入队报文资源统计等。
PDM: 决策将报文数据存储在IFB或者EFB，若决策为EFB的报文，则将报文从IFB中读出并写入EFB中。
QE: 为每个队列维护链表信息：头指针、尾指针、空闲状态等。每次报文入队时，更新队列链表的尾指针；每次报文出队时，更新队列链表的头指针。同时，还根据入队和出队状态，维护队列链表的空闲状态。
QS：主要完成多级调度和Shaping 操作。
 Work Flow
EQM的基本业务流程如下图所示：
Figure 5.8-2 LMEM High-Level Diagram
PE 的入队报文和EPS 的反馈的二次入队报文在EQM首先是要做入队判决，成功入队的
报文进行内外置缓存的选择，若是EFB 转发的报文需要将报文从IFB 搬到EFB 中，然后报文才进入正常的业务队列，并更新队列链表状态和空状态；如果是IFB 转发的报文，
则报文不需要搬，直接更新队列状态。入队失败的报文，向FAM释放缓存并统计丢包。报文出队时，发出报文的FD 给EPS。如果报文是二次入队报文，在EPS 从DDR 获得FD
信息后反馈报文信息给EQM进行二次入队，非二次入队的报文EPS 会正常转发到端口上。
5.8.5 Packet Buffer Manager（PBM）
 Buffer Resource Management
支持三种资源管理方式：基于报文个数管理、CELL 数管理、16BYTE 为单位的资源管
理。报文数管理和CELL 管理支持全局可配置，要么配置成报文数管理，要么配置成
CELL 管理,16B 缓存管理可以单独配置关闭。系统运行过程中BYTE 管理和CELL 管理
（或包管理）同时运行。下面的描述适合上述三种管理方式。
 芯片的队列资源分为两部分:队列独享资源和共享资源。
 队列独享队列资源是为队列预留的队列资源，即该队列至少可用的队列资源数目，即
使该队列不用，其它队列也不能占用。
 共享队列资源是为队列竞争使用的。一个报文申请入队，优先占用该队列的独享队列
资源。如果独享队列资源已经用完，再申请占用共享资源。返还的时候，优先返还共
享资源，之后返还独享资源。
 队列独享资源，共享资源之和必须小于等于芯片支持的最大队列资源数目。如果配置
不正确，则每个队列的独享队列资源得不到保证。
 队列基于模板可配独享队列资源和最大队列长度。
Figure 5.8-3 Reserved and Share Queue Resource
队列组的划分及配置:
 队列组是指一组队列，队列组的划分基于队列模板可配，即队列属于哪个队列组可配。
 具有队列组的属性，是指基于队列组的配置对缓存进行管理。
 软件可规划队列组的划分方法，逻辑不做限制。
队列组可配队列组最大共享资源group_max。灵活配置，既可限制队列组的最大共享资源占用，又可达到保留共享资源给队列组使用的效果。
Figure 5.8-4 Reserved and Share Queue Resource
队列组可配队列组芯片拥塞的丢弃门限chip_drop_group。灵活配置，可达到在拥塞情况
下，保留给优先权高，流量大的队列组共享资源的效果。
Figure 5.8-5 组芯片级共享资源门限示意图
 Tail-Drop
因为队列资源使用策略的配置，导致报文丢弃，称为尾丢弃。具体包括队列长度超过阈
值，或者独享队列资源已经用完，申请不到共享队列资源。
当队列发生尾丢弃之后，什么时候允许接收报文，基于队列模板可配。具体选择有：
 基于字节统计的队列深度值低于丢弃阈值；
 队列深度低于队列尾丢弃阈值；
 队列深度低于队列尾丢弃阈值的x/8，x 具体值在队列模板可配。
 Queue Resource Management
为了合理的利用队列资源，需要遵循的原则是尽量共享队列资源。通过拥塞管理，希望达到队列在芯片拥塞时，能够申请的缓存资源较少，反之能申请较多的缓存资源。
这里所说的队列资源是指报文的缓存资源。芯片对缓存资源的管理基即于CELL数目、也基于包数目管理，基于16BYTE级资源管理，允许预借资源。即每次判断资源数目时，用已占用资源数目进行判断。如果可以入队，则此报文必须能够全部入队，避免报文头可以接收，报文身体丢弃的决策。因为支持预借，这种情况下，芯片共享缓存需要再保留一些资源给预借，即此时队列独享资源，共享队列资源之和必须小于芯片支持的最大队列资源数目。
基于MCID管理的丢包策略。基于队列和出端口配置MCID的丢弃水线，当总的MCID占用达到某个队列或者出端口的丢弃水线，该队列或者出端口所属的报文开始丢弃。
队列缓存管理需要明确几个概念。
首先是状态信息的一些名词:
 q_depth:队列占缓存资源的深度
 group_depth:队列组占共享缓存资源的深度
 chip_depth:芯片占共享缓存资源的深度
下面是一些配置信息:
 q_private:保留给队列的独享缓存资源
 q_max:队列最大可用缓存数目
 q_cong:队列拥塞门限。队列占用缓存数目大于等于此值时；否则，认为队列不拥塞。
 group_max:队列组最大可用共享缓存数目
 chip_drop_group:基于芯片拥塞的队列组丢弃门限。即芯片占共享缓存超过此阈值时，丢弃相应队列组的报文。
 group_cong:队列组占用共享缓存数目大于等于此值时；否则，认为队列组不拥塞。
 chip_cong:芯片的拥塞阈值。即芯片占共享缓存资源超过此值认为芯片已经拥塞。
下面是具体的包丢弃管理方案
Figure 5.8-6 入队判决方案
上述算法适用于16B缓存管理方案、CELL管理方案、包管理方案。如果任何一套算法判决丢弃，则包应该丢弃。运行过程中，字节级管理方案、CELL管理方案（或包管理方案）任何一套方案判决为丢弃则该报文须丢弃。
 Multicast Buffer Calculation
当配置为基于CELL（或配置为PKT）、16BYTE单元进行缓存管理的时候，会统计每个报文占用的CELL数目和16BYTE单元数目。组播报文比较特殊，组播叶子需要缓存不同的报文头，但是报文身体共用一个。因此支持两种计算方法，全局可配:一种为组播叶子只统计报文头占的CELL数目和16BYTE单元数目，另外一种为组播叶子统计报文头和身子占的CELL数目和BYTE单元数目。
 Buffer Cell And Packet Statistic
由于出队的时候，16Byte ,CELL的计数释放需要等EPS返回的FD信息才释放，而PKT的计数释放是在出队时刻就完成了，因此两者的释放时刻不同。因此如果当前发流的情况下，CPU读取队列的16Byte,CELL,PKT，芯片的16Byte,CELL和PKT统计的时候，可能存在不一致情况。但是EQM能保证，如果停流的情况下，CELL和PKT的计数应该清零。
 Surplus Weight Release
PE 模块负责组播报文的MCID申请，在最好一份叶子报文时将组播MCID的冗余权重送到EQM进行释放；最后一份叶子报文有可能NP转发决策为丢弃，但仍然需要将其送到EQM进行冗余权重的释放。冗余权重的计算在PE完成，PE从first leaf 到last leaf数出一份组播里有多少叶子报文是正常转发的，然后用32减去正常转发的叶子分数（不算last leaf）就是冗余权重。
 IFB/EFB Selection
内外置缓存的选择，判断流程如下：
a：二次入队的报文按照一次入队的结果；
b：出端口（egrb）为eth口的JUMBO报文且uni_jumbo_sel配置为0，选择IFB缓存；
c：入队失败的报文，选择标记打上IFB标记；
d：根据buf_sel_mode配置，即自适应模式和强制模式选择：
1. 自适应模式
自适应模式具体的缓存选择的判断流程如下：
d1：如果buf_sel_mode为0，则：
e1：IFB和PLB的cell使用个数加上当前报文的cell个数是否大于plb_ifb_max_cfg配置值，如果大于，选择EFB缓存；
f1：IFB的cell使用个数加上当前报文的cell个数是否大于ifb_max_cfg配置值，如果大于，选择EFB缓存；
g1：组播报文且mc_buf_sel_efb配置为1，选择EFB缓存；
h1：组播报文第一份叶子且dqm_mc_cnt_vld配置为1，组播body的cell个数加上当前报文的body的cell个数是否大于dqm_mc_max_cfg配置值，大于则选择EFB缓存；
i1：组播报文非第一份叶子且dqm_mc_cnt_vld配置为1，组播body的cell个数是否大于dqm_mc_max_cfg配置值，大于则选择EFB缓存；
j1：根据ifb_adj_mode，自适应阶梯模式和自适应私有缓存模式选择：
1.1自适应阶梯模式
自适应阶梯模式的动态选择内置或者外置的算法如下：
出端口组的绑定关系见寄存器NNI_GP，ETH_GP，PIE_GP，PRBS_GP，WOE_GP。
出端口组的触发水线配置和退出水线配置 见寄存器PORT0_GP_TH，PORT1_GP_TH，PORT2_GP_TH，PORT3_GP_TH。
4个出端口组在IFB中占用cell的统计值为：port0_ifb_cnt~port3_ifb_cnt；
组播body的cell个数为mc_ifb_body_cnt，
如果dqm_mc_head_only配置为1，组播报文中portx_ifb_cnt只统计报文头的cell个数；
对应的阶梯值如下：
Port3的阶梯值：
port0_ifb_cnt + port1_ifb_cnt + port2_ifb_cnt + port3_ifb_cnt + mc_ifb_body_cnt；
Port2的阶梯值：port0_ifb_cnt + port1_ifb_cnt + port2_ifb_cnt + mc_ifb_body_cnt；
Port1的阶梯值：port0_ifb_cnt + port1_ifb_cnt + mc_ifb_body_cnt；
Port0的阶梯值：port0_ifb_cnt + mc_ifb_body_cnt；
如果dqm_mc_head_only配置为0，组播报文中portx_ifb_cnt统计报文头和body的cell个数
对应的阶梯值如下：
Port3的阶梯值：port0_ifb_cnt + port1_ifb_cnt + port2_ifb_cnt + port3_ifb_cnt；
Port2的阶梯值：port0_ifb_cnt + port1_ifb_cnt + port2_ifb_cnt；
Port1的阶梯值：port0_ifb_cnt + port1_ifb_cnt；
Port0的阶梯值：port0_ifb_cnt；
自适应阶梯模式判断流程如下：
j1.1：如果ifb_adj_mode为0，则：
k1.1：该报文所属的出端口组对应的阶梯值大于触发水线，选择EFB缓存；
l1.1：该报文所属的出端口组对应的阶梯值小于退出水线，选择IFB缓存；
m1.1：保持上一次报文自适应阶梯模式选择的结果，即如果是触发状态，则选择IFB缓存；如果是退出状态，则选择EFB缓存。
1.2自适应私有缓存模式
自适应阶梯模式的动态选择内置或者外置的算法如下：
出端口组的绑定关系见寄存器NNI_GP，ETH_GP，PIE_GP，PRBS_GP，WOE_GP。
出端口组的私有水线配置 见寄存器PORT0_GP_TH，PORT1_GP_TH，PORT2_GP_TH，PORT3_GP_TH。
自适应私有缓存模式的4个端口组的私有缓存计算方法类似，以port1举例；
端口组1的IFB 的cell个数为port1_ifb_cnt ，计算方法同自适应阶梯模式；
端口组x的私有cell个数为：
portx_pri_cnt = (portx_ifb_cnt>portx_gp_th)？portx_ifb_cnt：portx_gp_th
端口组1的预留私有总和为：
Ifb_port1_sum = port0_pri_cnt + port1_ifb_cnt + port2_pri_cnt + port3_pri_cnt + mc_ifb_body_cnt(仅在dqm_mc_head_only=1时)；
Ifb_plb_port1_sum = port0_pri_cnt + port1_ifb_cnt + port2_pri_cnt + port3_pri_cnt + mc_ifb_body_cnt(仅在dqm_mc_head_only=1时) + pqm_eqm_plb_cnt；
自适应私有缓存模式判断流程如下：
j1.2：如果ifb_adj_mode为1，则：
k1.2：该报文所属的出端口组对应的portx_ifb_cnt加上当前报文的cell个数小于等于该端口组的私有缓存portx_gp_th，选择IFB缓存；
l1.2：该报文所属的出端口组对应的ifb_portx_sum加上当前报文的cell个数大于ifb_max_cfg，选择EFB缓存；
m1.2：该报文所属的出端口组对应的ifb_plb_portx_sum加上当前报文的cell个数大于plb_ifb_max_cfg，选择EFB缓存；
n1.2：选择IFB 缓存。
2. 强制模式
强制模式的缓存选择流程如下所示：
d2：如果buf_sel_mode 为1，则：
e2：该报文所属的出端口组对应的portx_buf_sel 配置为1，选择IFB 缓存；
f2：选择EFB 缓存。
5.8.6 Internal Loop
为了提供用户级的Qos，需要层次化的调度拓扑，EQM采用二次入队的方法来实现这个
功能。PE 编辑后的报文进入EQM的队列，如果二次入队指示有效，则经过拥塞管理、
调度然后从PORT0 端口调度出队后进入EPS，EPS 读取二次入队所需要的队列信息后再
环回到EQM的入端口，再重新经过拥塞管理、调度发往真正的出端口。
功能示意图如下：
Figure 5.8-7 二次入队处理示意图
5.8.7 Packet Data Manager
PDM子模块根据报文的转发出端口 进行分组管理，可以配置上行报文使用EFB缓存报文，下行报文使用IFB缓存报文，这样可以减少上行报文因为写DDR的时延大而阻塞下行的要缓存到IFB的报文，可以更充分地利用IFB的带宽，减轻EFB的带宽负荷。
 PDM 子模块负责EFB缓存地址的申请使用。当报文选择缓存到EFB时，PDM向FAM申请EFB CELL地址， PDM模块将报文数据写入相应的CELL地址中，同时完成帧内链的链接。
 PDM 子模块决策报文是缓存在IFB还是EFB， 有两种模式选择内外置缓存，采用哪种模式是配置决定的。模式1（自适应模式）：根据 端口组IFB缓存的占用情况，优先使用IFB，IFB超过端口组的IFB占用水线时使用EFB缓存报文；模式2（强制模式，仅作为Debug使用）： 根据报文的出端口和通道组配置，将报文分成4个组，按组固定配置使用IFB或EFB；
 如果报文选择缓存在EFB，则PDM模块将报文数据从PLB搬到EFB，然后将FD送往QE子模块入队。如果报文选择缓存在EFB，则不需要搬动报文，只需要将报文的所占的PLB资源统计转为IFB资源统计，然后将FD送往QE。
 当配置使用自适应模式缓存选择，出端口总共有4组，对应的优先级别从port0～port3递减，使用4个出端口IFB资源使用统计，port_cnt0~port_cnt3另外还有一个组播身体专用的cell使用资源统计mc_cnt。
 当报文为单播报文的时候，各个出端口组的要求如下：
 对于port3要求：mc_cnt+port0_cnt+port1_cnt+port2_cnt+port3_cnt <=port3_gp_th才能走IFB通道
 对于port2要求：mc_cnt+port0_cnt+port1_cnt+port2_cnt <=port2_gp_th才能走IFB通道；
 对于port1要求：mc_cnt+port0_cnt+port1_cnt <=*port1_gp_th才能走IFB通道；
 对于port0要求：mc_cnt+port0_cnt <=port0_gp_th才能走IFB通道；
Figure 5.8-8 内外置缓存选择
5.8.8 Queue Schedule
. Schedule Topology Overview
. EQM可以支持业务/用户/端口调度。同时支持基于逻辑通道的反压。
. 调度器拓扑结构示意图（调度拓扑可配置）如下:
Figure 5.8-9 SCHEDULER TOPOLOGY 示意图
 Schedule Output
 EQM的调度输出支持基于逻辑通道的反压,PE 的入队和EPS 的二次入队请求首释
放通道FIFO 的将满反压。逻辑通道EQM_CHID 的具体含义见错误!未找到引用
源。EQM _CHID 说明章节。
 EPS 需要周期把逻辑通道的状态送给EQM，EQM把它映射到调度器的输出。受反
压的调度器输出不能送给EPS。
 EPS 送给EQM的反压状态有一定的延时。因此，允许反压产生之后，EQM还送
出1~2 个帧描述符FD 给EPS。
 同一个端口，最快16 拍送出一个FD；
 不同端口之间，最快可以8 拍输出一个FD；
 Port Schedule
逻辑支持4 个端口调度器。为了节省模块间的布线资源，需要时分复用一组总线，把调度
结果送给后序模块。端口之间进行RR 轮询。
Figure 5.8-10 Port Schedule
 Shaper
 EQM的实体队列、调度器的输出都有shaper 进行限速。
 SHAPER 支持预借令牌。最多预借一个报文的令牌。
 基于实体队列的shaper:每个队列可配绑定支持一个shaper，即同时支持的shaper
数目跟实体队列数目一致。当shaper 令牌不够时，相当于一级调度器看不到此实
体队列的调度请求。
 通用调度器的shaper:每个通用调度器可配绑定支持一个shaper，即同时支持的
shaper 数目跟通用调度器的数目一致。当shaper 令牌不够时，相当于通用调度器没
有调度结果。
 Schedule Algorithm
EQM同时支持RR、 SP/WRR 等调度算法。下面详细解释各个算法的特点和用法。
Strict Priority：
Strict Priority(SP)调度即严格优先级调度，是基于优先队列（Priority Queuing，
PQ）的一种调度算法。该算法按照队列优先级的高低进行调度，高优先级先调度，低优
先级后调度.高实体队列非空，就先调度优先级高的队列出队，只有在优先级高的队列调
度空后，才调度低实体队列。如下图所示，从Queue0 到Queue3 也按照优先级的降序做
绝对优先调度，在报文出队的时候，SP 首先让处于最高优先级的Queue0 队列中的报文出
队，直到Queue0 队列中的报文发送完，然后发送中优先队列中的报文，同样直到发送
完，依次类推，只有Queue0、Queuel 和Queue2 队列的报文全部出队才去调度Queue3 队
列。只要高实体队列非空就一直调度高实体队列， 优先级级别越低的队列得到调度机会
就越少 (Queue0>Queuel>Queue2>Queue3)。
Figure 5.8-11 Strict Priority
Round Robin：
Round Robin (RR),也就是轮流调度各个输入端。主要适用于对帧长不敏感，各个输入端权
重一样的调度。
Weight Round Robin：
WRR 调度要求定义一个数值用于规定当前队列与其他优先级队列的相对重要性
（weight）。WRR 调度防止低优先级的队列在高优先级队列传输时被完全忽略。WRR 调
度对各个队列实行轮流发送机制。报文的权重与队列的重要性相对应。通过调度功能，即
使高优先级的队列为非空，低优先级的队列也能获得机会发送报文，这样带宽资源可以得
到充分的利用。
SP/WRR:
SP/WRR 是SP 和WRR 混合调度，如下图所示。对每个输入端可配组属性及WRR 调度权
重。逻辑对同一组的输入端进行WRR 调度，然后对各个组进行SP 调度。进行SP 调度时
候，0 优先权最高，7 优先权最低。
Figure 5.8-12 SP/WRR
 Share-Bandwidth Schedule
共享带宽设计需要支持的目标：
支持用户间的带宽保障
支持用户业务间的带宽保障。
支持用户间高优先级业务的带宽保证。
场景1（多用户带宽保证）：
总上行带宽20Mbps。
如果只有用户A 上网，则A 独占20Mbps 带宽。
如果用户B 加入，则A/B 各分配最低5Mbps 保证带宽，A 优先级比B 高，可以优先级占
用10Mbps 的保留带宽，A 最高15Mbps 最大带宽。
如果A 和B 同时在线，则A/B 各分配最低5Mbps 保证带宽，则剩余的10Mbps 带宽可以
按比例分配给A 和B。
场景2（多业务带宽保证）：
总上行带宽20Mbps。
如果用户只有上网业务，则独享20Mbps 带宽。
如果用户有视频和上网业务，视频和上网各配置5Mbps 的保证带宽，则视频优先级高可
以分配到15Mbps 带宽，上网分配5Mbps。
如果用户有视频和上网业务，视频和上网各配置5Mbps 的保证带宽，则剩余的10Mbps 带
宽可以按比例分配给视频和上网。
场景3（多用户多业务带宽保证）：……
用户A 在玩即时游戏，用户B 在做FTP 下载。
则用户A 优先使用带宽，用户B 低优先级使用带宽。
Figure 5.8-13 典型调度拓扑
QoS 需要支持如上图所示的拓扑结构：用户业务队列，用户队列，和端口优先级队列。
从外到内，
. 第一级为端口优先级队列，支持4/8个业务优先级。
. 第二级为用户队列。
 第三级为用户业务队列，每个用户4/8个优先级队列。
实现方案：
图表 10 共享带宽令牌
A.保证带宽令牌桶：
各种业务的保证令牌根据配置的xir 配置的大小，按照固定的125us 周期往令牌桶里添加
令牌。如果业务流量少于保证带宽，令牌桶慢慢就会被填满，周期性添加令牌时就会有溢
出，溢出部分的令牌就会被添加到共享令牌桶中。
B.共带宽令牌桶：
共享保证令牌桶根据配置的保留带宽配置的大小，按照固定的125us 周期往令牌桶里添加
令牌。如果各个保证带宽令牌有溢出，溢出部分的令牌会加上周期性添加的令牌一起添加
到共享令牌桶中。
调度:
在有保证带宽令牌的情况下，优先使用保证带宽令牌桶里的令牌，消耗共享令牌阶段采用
RR 调度，确保各业务流的保证带宽能实施。
在调度器各个输入的保证带宽都消耗完或队列空的情况下，根据保留带宽的分配策略来调
度；若按照优先级来分配保留带宽则此阶段进行SP 调度，若按照比例来分配保留带宽，
则此阶段进行WRR 调度。
1. 仅0~7号通用调度器支持共享带宽特性；耗不完的保证带宽会转为共享带宽，共享带宽的根据配置采用SP调度或RR调度。
2. 将GSSA表中的specific_sch_sel配置成2‘b10，共享带宽按照优先级分配，调度器输入端抢占共享带宽的优先级在GSSA表中配置。
3. 将GSSA表中的specific_sch_sel配置成2‘b11，共享带宽按照比例权重分配，各输入端的抢占共享带宽权重在STS表中配置。
4. 使用该共享带宽调度时，调度器各个输入端的保证带和共享带宽的BWT表中的bw_ctrl_en配置成1；
5. 使用共享带宽特性的调度器仅支持0~7 8个输入端。
6. 保证带宽的配置在BWT表中0~63的表项中配置，譬如调度器0的输入2的保证带宽是在BWT的地址0x02上配置；调度器3的输入1的保证带宽是在是在BWT的地址0x31上配置；
7. 0~7共享带宽的配置在BWT表中地址为64~71的表项中配置，譬如调度器0在BWT的地址64上配置；调度器7的共享带宽是在BWT的地址71上配置；
8. 使用共享带宽特性的调度器仅支持0~7 8个输入端。
9. 保证带宽的XBS建议配置成2倍的速率配置值，譬如保证带宽10Mbps,则XBS建议配置成MAX {MTU，2*10_000/64 Byte}。
10. 共享带宽的XBS至少需要配置成调度器总带宽的匹配值的2倍，譬如调度器有4个输入端，每个输入端的保证带宽为20Mbps,共享带宽为10Mbps, 则共享带宽的XBS至少需要配置成MAX{MTU，2* 90000/64 Byte}。
注：
1. 多级调度时，仅支持其中一级调调度器支持共享带宽特性，同一调度收敛树上不支持多级调度同时使用共享带宽特性。
2. 使用共享带宽特性的调度器不支持传统的shaping功能。(调度器的输入支持shaping, 调度器的输出不支持shaping)
5.8.9 Error Handling
 缓存资源占用量超过配置的水线时，会上报中断。
 缓存资源占用量减溢出时，会记录历史状态，无恢复措施。
 调度拓扑配置错误时，会记录错误状态。
 RAM校验错误时，会记录错误状态。
5.8.10 Debug Features
EQM内部状态，CPU可读。如果配置寄存器EQM_DEBUG_CTRL，打开调试开关TAB_DEBUG，则状态表CPU也可以配置进行调试。
 A. 支持调度拓扑连接错误的告警可查询；
 B. 支持调度错误（调出空队列）告警；
 C. 支持缓存资源占用超过总容量的告警；
 D. 支持模块级收发包统计，拥塞管理丢包，异常丢包统计可查询；
 E. 支持按队列入队和丢包统计，丢包包括：尾丢弃，拥塞丢弃；
 F.支持实时查询芯片当前队列缓存的包数目和CELL数目；
 G.支持单包出队调试模式；
5.8.11 Power Management
EQM 统计二次入队队列里攒着的报文，如果队列报文统计不为0，则输出1bit busy信号有效，否则输出busy 信号无效。当Busy 信号有效时，申请高档位的频率，当Busy信号无效时，申请低档位的频率。
5.8.12 Table Configuration
因支持的队列和队列资源数目较多，很多控制信息都是存储在内部表项中。表项采用间接访问方式，可以节省地址访问空间，有利于各个项目重用。
下面的描述需要操作寄存器EQM_TAB_CFG，EQM_TAB_RDATA0~5，EQM_TAB_WDATA0~5，具体请参看寄存器描述。
 Table Write
具体步骤如下:
 查询寄存器EQM_TAB_CFG，等待上次操作完成。
寄存器EQM_TAB_CFG. TAB_OP_START为0，表示上次操作已经完成；为1，说明CPU上次操作还没有完成，需要继续等待，直到上次操作完成。
 配置寄存器EQM_TAB_WDATA0~5。
根据所配置的表项的位宽，决定需要配置的寄存器数目。最终EQM会把{ EQM_TAB_WDATA5，EQM_TAB_WDATA4, EQM_TAB_WDATA3, EQM_TAB_WDATA2，EQM_TAB_WDATA1, EQM_TAB_WDATA0}写进相应表项的对应地址。如果相应TAB的位置不足192位，则取低位的值配置。如调度器属性表GSSA，位宽为96bit，如果配置表GSSA，则逻辑会把{ EQM_TAB_WDATA2[31:0], EQM_TAB_WDATA1, EQM_TAB_WDATA0}写进相应TAB的对应地址。
 配置寄存器EQM_TAB_CFG，发动写操作。
具体为EQM_TAB_CFG.TAB_OP_START为1，EQM_TAB_CFG.TAB_OP_WR为1，EQM_TAB_CFG.TAB_IDX/ EQM_TAB_CFG.TAB_ADDR为所需要配置的TAB的编号和具体地址。
 查询寄存器EQM_TAB_CFG，等待这次操作完成。
寄存器EQM_TAB_CFG. TAB_OP_START为0，表示上次操作已经完成；为1，说明CPU上次操作还没有完成，需要继续等待，直到上次操作完成。
 Table Read
具体步骤如下:
 查询寄存器EQM_TAB_CFG，等待上次操作完成。
寄存器EQM_TAB_CFG. TAB_OP_START为0，表示上次操作已经完成；为1，说明CPU上次操作还没有完成，需要继续等待，直到上次操作完成。
 配置寄存器EQM_TAB_CFG，发动读操作。
具体为EQM_TAB_CFG.TAB_OP_START为1，EQM_TAB_CFG.TAB_OP_WR为0，EQM_TAB_CFG.TAB_IDX/ EQM_TAB_CFG.TAB_ADDR为所需要配置的TAB的编号和具体地址。
 查询寄存器EQM_TAB_CFG，等待这次操作完成。
寄存器EQM_TAB_CFG. TAB_OP_START为0，表示上次操作已经完成；为1，说明CPU上次操作还没有完成，需要继续等待，直到上次操作完成。
 读寄存器EQM_TAB_RDATA0~5。
根据所配置的TAB的位宽，决定需要读取的寄存器数目。
如调度器属性表QGA，位宽为96bit，则逻辑会把长字2写入寄存器EQM_TAB_RDATA2中，长字1写入寄存器EQM_TAB_RDATA1中，长字0写入寄存器EQM_TAB_RDATA0中。
 Dfx of Table Access
CPU操作可能会因为以下几个原因导致出错:
 EQM_TAB_CFG.TAB_IDX配置错误。即如果CPU访问内部不存在的TAB。
 对内部状态TAB进行写操作，但是没有把相关使能打开。
EQM内部所有的状态表，在正常模式下，CPU是只读的。如果要进行写操作，则需要把相应的使能打开，具体为把寄存器的EQM_DEBUG_CTRL . TAB_DEBUG配置为1。
 CPU操作TAB出错的表现和解决方法
一旦CPU操作TAB出错，逻辑会马上返还操作完成并报错。具体表现为EQM_TAB_CFG.TAB_OP_ERR为1。软件需要检查自己的操作命令，修改正确后，再发动一次正确的操作。
